# -*- coding: utf-8 -*-
"""
Created on Thu Apr 30 16:54:26 2024

@author: AlexYu
"""

#%%  
"""
Converts all PDF files in a folder to txt files
"""
import os
from pdfminer.high_level import extract_text
import re
import logging

# Setting up logging to output the status of file conversion
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def pdf_to_markdown(pdf_path):
    try:
        text = extract_text(pdf_path)
        markdown_text = re.sub(r'\n{2,}', '\n\n', text)  # Ensure proper paragraph spacing
        markdown_text = re.sub(r'\n', ' ', markdown_text)  # Convert single newlines into spaces
        markdown_text = re.sub(r'\s{2,}', ' ', markdown_text)  # Remove extra spaces
        return markdown_text
    except Exception as e:
        logging.error(f"Failed to process {pdf_path}: {e}")
        return None

def convert_folder_to_markdown(pdf_dir):
    for filename in os.listdir(pdf_dir):
        if filename.endswith(".pdf"):
            pdf_path = os.path.join(pdf_dir, filename)
            txt_path = pdf_path.replace('.pdf', '.txt')

            # Check if the txt file already exists
            if os.path.exists(txt_path):
                logging.info(f"Skipping conversion, text file already exists: {txt_path}")
                continue  # Skip this file as it has already been converted

            markdown_text = pdf_to_markdown(pdf_path)
            if markdown_text is not None:
                try:
                    with open(txt_path, 'w', encoding='utf-8') as md_file:
                        md_file.write(markdown_text)
                    logging.info(f"Converted {pdf_path} to {txt_path}")
                except Exception as e:
                    logging.error(f"Could not write to {txt_path}: {e}")

# Specify the folder containing PDFs
pdf_dir = r"your file location"
convert_folder_to_markdown(pdf_dir)

#%%
"""
load txt documents from a specified directory, split them into smaller chunks based on certain criteria, 
and then save those chunks into a Chroma vector database using embeddings generated by OpenAI's models. 
"""
import os
import shutil
import time
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores.chroma import Chroma

CHROMA_PATH = "chroma"
DATA_PATH = r"your file path that stores the txt files for Retrieval"

def main():
    generate_data_store()

def generate_data_store():
    documents = load_documents()
    chunks = split_text(documents)
    save_to_chroma(chunks)

def load_documents():
    loader = DirectoryLoader(DATA_PATH, glob="*.txt")
    documents = loader.load()
    return documents

def split_text(documents: list[Document]):
    text_splitter = RecursiveCharacterTextSplitter(
        # separators=[
        # "\n\n",
        # "\n",
        # " ",
        # "."],
        chunk_size=300,
        chunk_overlap=100,
        length_function=len,
        add_start_index=True,
    )
    chunks = text_splitter.split_documents(documents)
    print(f"Split {len(documents)} documents into {len(chunks)} chunks.")

    document = chunks[10]
    print(document.page_content)
    print(document.metadata)

    return chunks

def safe_remove(path):
    """Attempt to remove a path with retries."""
    max_attempts = 3
    for attempt in range(max_attempts):
        try:
            shutil.rmtree(path)
            print(f"Successfully removed {path}")
            break
        except PermissionError as e:
            print(f"Attempt {attempt + 1} failed: {e}")
            time.sleep(1)  # Wait a bit for the file to be released
        if attempt == max_attempts - 1:
            print(f"Failed to remove {path} after {max_attempts} attempts.")

def save_to_chroma(chunks: list[Document]):
    # Clear out the database first.
    safe_remove(CHROMA_PATH)

    # Create a new DB from the documents.
    openai_key = "Your OpenAI API key"  # Replace this with your actual API key
    db = Chroma.from_documents(
        chunks, OpenAIEmbeddings(openai_api_key=openai_key), persist_directory=CHROMA_PATH
    )
    db.persist()
    print(f"Saved {len(chunks)} chunks to {CHROMA_PATH}.")

if __name__ == "__main__":
    main()
#%% 
"""
To perform a specific text-based query against a vector database and use the results to generate a response using 
an AI-driven chat model
"""
from langchain.vectorstores.chroma import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
import os 

CHROMA_PATH = "chroma"
PROMPT_TEMPLATE = """

Answer the question based only on the following context:

{context}

---

Answer the question based on the above context: {question}
"""

def main():
    # Your OpenAI API key
    api_key = "Your OpenAI API key"  # Replace this with your actual API key

    # Prepare the DB.
    embedding_function = OpenAIEmbeddings(openai_api_key=api_key)
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)

    #########################################################################################################
    query_text = "does covered call supports put option? explain in detail" # Your query text here
    #########################################################################################################
    
    # Search the DB.
    results = db.similarity_search_with_relevance_scores(query_text, k=20) # Control your chunks for inquiry here
    if len(results) == 0 or results[0][1] < 0.7:
        print("Unable to find matching results.")
        return

    context_text = "\n\n---\n\n".join([doc.page_content for doc, _score in results])
    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
    prompt = prompt_template.format(context=context_text, question=query_text)
    print(prompt)

    # Initialize the model and generate response
    model = ChatOpenAI(openai_api_key=api_key)
    response_text = model.predict(prompt)


    # Collect and print sources, preserving order and removing duplicates
    seen = set()
    sources = []
    for doc, _score in results:
        source = os.path.basename(doc.metadata.get("source", ""))
        if source not in seen:
            seen.add(source)
            sources.append(source)

    formatted_response = f"Response: {response_text}\nSources: {sources}"
    print(formatted_response)

if __name__ == "__main__":
    main()
    